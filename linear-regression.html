<!DOCTYPE html> <html> <head> <link rel=apple-touch-icon-precomposed sizes=152x152 href="apple-touch-icon-152x152-precomposed.png"/> <link rel=apple-touch-icon-precomposed sizes=144x144 href="apple-touch-icon-144x144-precomposed.png"/> <link rel=apple-touch-icon-precomposed sizes=114x114 href="apple-touch-icon-114x114-precomposed.png"/> <link rel=apple-touch-icon-precomposed sizes=72x72 href="apple-touch-icon-72x72-precomposed.png"/> <link rel=apple-touch-icon-precomposed href="apple-touch-icon-precomposed.png"/> <link rel="shortcut icon" href="favicon.png"/> <link rel=icon type="image/ico" href="favicon.ico"/> <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,300' rel=stylesheet> <link rel=stylesheet href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css"> <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script> <meta name=viewport content="width=device-width, initial-scale=1.0"> <link href="stylesheets/main-07333310.css" rel=stylesheet /> <link href="stylesheets/highlighting-33a23ae7.css" rel=stylesheet /> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120236812-1"></script> <script>
  function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-120236812-1");
</script> </head> <div class=date> 2019-01-18</div> <div class=row> <section class=article-container> <h1 id=linear-regression---behind-the-scenes>Linear Regression - behind the scenes</h1> <h2 id=which-problem-do-we-want-to-solve>Which problem do we want to solve?</h2> <p>We are given some data points <script type="math/tex">{(x_1,y_1), (x_2,y_2), (x_3,y_3), (x_n,y_n)}</script> which we call our training set. Then we get a new data point <script type="math/tex">x_{new}</script> for which we want to predict the corresponding <script type="math/tex">y_{new}</script> value in a way that it follows the same pattern as in our training set. Thus our task is to find a model which can capture/learn these underlying patterns (aka relationships) in our training set and apply them to new points to make accurate predictions.</p> <p>Some models types can capture/learn only a specific type of pattern (e.g. Linear Regression), others are more powerful and can learn any type of pattern (e.g. Neural Networks). Unfortunately the more powerful models will not only learn useful patterns but also the noise in our data. Usually it’s best to pick the simplest model type which is yet able to learn the most important patterns in your data because the simpler the model is the less chance there is that it learns noise.</p> <h2 id=implicit-model-assumption-for-linear-regression>Implicit model assumption for Linear Regression</h2> <p>Linear Regression is a simple model type which assumes that the relationship between <script type="math/tex">x</script> and <script type="math/tex">y</script> is linear. Because of the added bias term the relationship it is strictly speaking affine and not linear. But you can apply a neat trick to make it linear again.</p> <p><strong>For scalar valued <script type="math/tex">x</script>:</strong><br/> <script type="math/tex">y = w_1*x_1 + bias</script></p> <p><strong>For vector valued <script type="math/tex">x</script>:</strong><br/> <script type="math/tex">y = w_1*x_1 + w_2*x_2 + w_3*x_3 + bias = w^{T}x + bias</script></p> <p><strong>Trick: affine =&gt; linear</strong><br/> <script type="math/tex">y = w_1*x_1 + w_2*x_2 + w_3*x_3 + w_4*(+1) = w^{T}x</script><br/> By adding a new entry <script type="math/tex">+1</script> to every data point <script type="math/tex">x</script> we can absorb the bias term (<script type="math/tex">w_4*1 = bias</script>) and make the relationship between <script type="math/tex">x</script> and <script type="math/tex">y</script> linear.</p> <h2 id=what-if-the-assumption-doesnt-hold>What if the assumption doesn’t hold?</h2> <p>Here are three examples of how the relationship between x and y might look like. The relationship can either be <strong>exactly linear</strong>, <strong>roughly linear</strong> or <strong>not linear at all</strong>. <img src="images/linear_regression/assumptions-24a49f19.png" alt=linear_regression width=1169 height=441 /></p> <p>We can fit a linear regression model in any case but the predictions won’t make much sense in the case when the data doesn’t follow a linear relationship.</p> <p><img src="images/linear_regression/assumptions_with_line-d26deafb.png" alt=linear_regression width=1169 height=441 /></p> <h2 id=how-can-we-find-the-best-model-paramter-w>How can we find the best model paramter w?</h2> <ol> <li>You define a function which tells you how good your current estimate of the best paramters <script type="math/tex">w</script> is. This function is usually called <strong>error function</strong> or <strong>loss function</strong>.</li> <li>You minimize the error function by changing your paramter values <script type="math/tex">w</script>. When people say they fit a model they refer to this step of minimizng an error function. For some model types and error functions there exist a closed form solution for <script type="math/tex">w</script> which minimizes the error function for others you need to apply other (often iterative) minimization methods, e.g. for logistic regression.</li> </ol> <h3 id=error-meassure-1-least-square-error>Error meassure 1: least square error</h3> <script type="math/tex; mode=display">% <![CDATA[
\begin{split}
E(w)& =\frac{1}{2} \sum\nolimits_{n=1}^N (w^{T}x_i - y_i)^2 \\
 & = \frac{1}{2}(Xw-Y)^{T}(Xw-y) \\
 & = \frac{1}{2} (w^TX^T-y^{T})(Xw-y) \\
 & = \frac{1}{2}(w^TX^TXw -w^TXy - Y^TXw - Y^{T}y) \\
 & = \frac{1}{2}(w^TX^TXw - 2w^TX^Ty - Y^{T}y) \\
\end{split} %]]></script> <p>The vectors and matrices have the following dimensions.<br/> <script type="math/tex">w</script>: Dx1 <br/> <script type="math/tex">x_i</script>: Dx1<br/> <script type="math/tex">y_i</script>: 1x1<br/> <script type="math/tex">y</script>: Nx1<br/> <script type="math/tex">X</script>: NxD</p> <p>In order to minizme the error function we take the gradient with respect to <script type="math/tex">w</script> set it to zero and solve for <script type="math/tex">w</script>.</p> <script type="math/tex; mode=display">% <![CDATA[
\begin{split}
& \nabla_{w} E(w) =  0 \\
& \Rightarrow\quad \nabla_{w} \frac{1}{2}(w^TX^TXw - 2w^TX^Ty - y^{T}y) = 0 \\
& \Rightarrow\quad X^{T}Xw = X^{T}y \\
& \Rightarrow\quad X^{T}Xw = X^{T}y \\
& \Rightarrow\quad w* = (X^{T}X)^{-1}X^{T}y \\
\end{split} %]]></script> <h3 id=error-meassure-2-least-absolute-error>Error meassure 2: least absolute error</h3> <p><script type="math/tex">E(w) = \sum\nolimits_{n=1}^N |w^{T}x_i - y_i|</script></p> <p>There exists no closed form solution for the least absolute error because of the absolute value in the equation. That’s why an iterative approach like Gradient Descent is required which won’t be introduced at this point.</p> <p>For the first plot (above) the least square error was used to fit the model, for the plot below the least absolute error was used. The results are pretty much the same.</p> <p><img src="images/linear_regression/assumptions_with_line_mae-1e4b93d6.png" alt=linear_regression width=1169 height=441 /></p> <h2 id=basis-functions---how-to-deal-with-nonlinear-data>Basis functions - how to deal with nonlinear data</h2> <p>In case the relationship between <script type="math/tex">x</script> and <script type="math/tex">y</script> is not linear you have two options. You can either switch to a completely new type of model which is able to capture nonlinear relationships or you can transform your data into a new space where the relationship between your transformed data x and y is linear again. The function which transforms your data is generally called a <strong>basis function</strong>.</p> <p><strong>Example 1:</strong></p> <p>Let’s assume we have a quadratic relationship between <script type="math/tex">x</script> and <script type="math/tex">y</script> in the form of <script type="math/tex">y = 1.5x^2</script>. If we transform our data to a new space (again just a 1D space) with the new x axis <script type="math/tex">x_{transformed} = x^2</script> then the relationship between <script type="math/tex">y</script> and <script type="math/tex">x_{transformed}</script> will be linear again <script type="math/tex">y = 1.5x_{transformed}</script> as you can see in the plot below.</p> <p><img src="images/linear_regression/basis_function-60eeecc5.png" alt=linear_regression width=1167 height=442 /></p> <p><strong>Example 2:</strong></p> <p>Usually you don’t know to which space you should transform your data in order to make the raltionship linear again. To overcome this issue you can transform your data to a very high dimensional space and let your model decide which transformations to use.</p> <p>For example assume the true relationship between <script type="math/tex">y</script> and <script type="math/tex">x</script> has the form of <script type="math/tex">y = 1.5x^3</script> but you don’t know it in advance. Now you transform your data to a high dimensional space with the new axes <script type="math/tex">x_{transformed 1} = x^2</script>, <br/> <script type="math/tex">x_{transformed 2} = x^3</script>, <br/> <script type="math/tex">x_{transformed 3} = x^4</script>,<br/> <script type="math/tex">x_{transformed 4} = x^5</script>, <br/> <script type="math/tex">x_{transformed 5} = x^6</script>.</p> <p>This works well in theory as your model can always decide not to use a transformation by setting the corresponding model parameter to <script type="math/tex">0</script>.</p> <script type="math/tex; mode=display">% <![CDATA[
\begin{split}
y &= w_1x_{transformed 1} + w_2x_{transformed 2} + w_3x_{transformed 3} + w_4x_{transformed 4} + w_5x_{transformed 5} \\
&= 0x_{transformed 1} + 1.5x_{transformed 2} + 0x_{transformed 3} + 0x_{transformed 4} + 0x_{transformed 5} \\
&= 0x^2 + 1.5x^3 + 0x^4 + 0x^5 + 0x^6
\end{split} %]]></script> <p><strong>Example 3:</strong></p> <p>Using this technique you can fit your model to data which only approximately follows a polynomial relationship like for example data which was generated in the follwing way: <script type="math/tex">y = sin(x)</script>. If we use the same transformation technique up to an exponent of 5 we get the following model when we fit our model with a least square error.</p> <script type="math/tex; mode=display">y = 0.86x -0.0000000000000000018x^2 -0.12x^3  + 0.000000000000000064x^4
  + 0.0028x^5</script> <p>You can see in the plot below that our model captures the data pretty well and it allows us to make good predictions.</p> <p><img src="images/linear_regression/sin_example-466eee1b.png" alt=linear_regression width=1185 height=441 /></p> <p><strong>Example 4:</strong></p> <p>So far it seems best to transform the data to an as high dimensional space as possible and let your model figure out which transformations to choose. But there is a drawback. Assume that your data was generated in the following way <script type="math/tex">y = x^2 + noise</script>. As before let’s transform our data up to an exponent of 40 and see what happens.</p> <p><img src="images/linear_regression/overfitting_example-fc9b4955.png" alt=linear_regression width=1167 height=441 /></p> <p>In red you see the fitted model which uses the transformed high dimensional data and in green our desired model. This phenomen is called overfitting. Our model is to complex and uses the extra power (high dimensional features e.g. <script type="math/tex">x^{40}</script>) to follow not only the major underlying quadratic pattern but also the noise.</p> <p>In conclusion, you can say that our model is <strong>not</strong> able to figure our itself which transformations to use. When we provide higher dimensional features to our model it will <strong>not</strong> ignore the ones which are not necessary to describe the major underlying pattern but rather use them to also capture noise in our data. We somehow need to find the sweet spot of complexity where our model is powerful enugh to capture the important patterns but not powerful enough to also capture the noise. There is one technique called <strong>Regularization</strong> which can help us with that.</p> <h2 id=regularization>Regularization</h2> <p>Regularization describes methods which prevent our model from overfitting by reducing the complexity of your model. This is a quite a general definition but provides you with the right intuition for future more complex models where regularization can occure in many different ways and not just by tampering the error function which we will do in a second for linear regression.</p> <h3 id=l2-regularization-for-linear-regression>L2 Regularization for Linear Regression</h3> <script type="math/tex; mode=display">E(w) = \sum\nolimits_{n=1}^N (w^{T}x_i - y_i)^2 + \frac{\lambda}{2}\lVert w\rVert_2^2 = (Xw-y)^T(Xw-y) + \frac{\lambda}{2}w^Tw</script> <p>L2 regularization adds a new term to the error function which prevents the parameters w to become too large. It uses the squared euclidean distance/squared L2 norm to do so. Let’s again minimize the error function with respect to <script type="math/tex">w</script>.</p> <script type="math/tex; mode=display">% <![CDATA[
\begin{split}
& \nabla_{w} E(w) =  0 \\
& \Rightarrow\quad \nabla_{w} (Xw-y)^T(Xw-y) + \frac{\lambda}{2}w^Tw = 0 \\
& \Rightarrow\quad X^{T}Xw - X^{T}y + \lambda w= 0 \\
& \Rightarrow\quad (X^{T}X-\lambda I)w = X^{T}y \\
& \Rightarrow\quad w* = (X^{T}X -\lambda I)^{-1}X^{T}y \\
\end{split} %]]></script> <h3 id=l1-regularization-for-linear-regression>L1 Regularization for Linear Regression</h3> <p><script type="math/tex">E(w) =\sum\nolimits_{n=1}^N (w^{T}x_i - y_i)^2 + \lambda\lVert w\rVert_1 = 
(Xw-y)^T(Xw-y) + \lambda\lVert w\rVert_1</script></p> <p>L1 regularization also adds a new term to the error function which pretty much does the same thing as the l2 regularization but uses the L1 norm (not squared) instead.</p> <p>Let’s look at an example similiar to the previous one. Our data was generated in the following way: <script type="math/tex">y = 8x^2 + 20sin(4x) + noise</script>. I played around with the coefficients to make the plots look nice. Here are the results.</p> <p><img src="images/linear_regression/regularization-23970b94.png" alt=linear_regression width=1173 height=441 /></p> <p>Firstly, you can see that a model (red line) with regularization becomes smoother compared to a model without regularization. Secondly, you can see that L1 regularization leads to an even “smoother” model than L2 regularization. Why is this so? The L2 norm doesn’t penalize all errors in the same way. For example when your model is far off <script type="math/tex">w^Tx >> y</script> you will get a huge error because of the quadratic term in the L2 norm. However when you are only slighly off <script type="math/tex">w^Tx ~ y</script> then your error will be almost zero again because of the quadratic term. This is different in the L1 norm where you use the absolute value of the distance. That’s why L1 regularization can push parameters w down to zero whereas L2 regularization will make them small but not zero.</p> <p>The following picture provides another intuitive way to grasp what regularization for linear regression does. Essentially regularization offers a way to shrink the complexity of our model without changing its structure.</p> <p><img src="images/linear_regression/complexity-038a7839.png" alt=linear_regression width=590 height=560 /></p> <h2 id=behind-the-scenes-probabilistic-view>Behind the scenes: probabilistic view</h2> <p>Now we come to the fun part where we will explore the connection between a regularized non probabilistic model and a probabibilistic model with certain assumptions.</p> <p>Our probabilistic model: <script type="math/tex">y_i = w^Tx_i + N(\epsilon|0,\sigma^2)</script></p> <p>As the noise is the only probabilistic term it follows directly: <script type="math/tex">p(y_i|x_i,w,\sigma^2) = N(\epsilon|w^Tx_i,\sigma^2)</script></p> <p>It’s a bit confusing at first to read those formulas. In our case we interpret <script type="math/tex">p(y_i|x_i,w,\sigma^2)</script> not as a function of <script type="math/tex">y_i</script> but as a function over <script type="math/tex">w</script> and <script type="math/tex">\sigma^2</script>. This interpretation of the formula is generally called a likelihood function. The integral over <script type="math/tex">w</script> and <script type="math/tex">\sigma^2</script> doesn’t sum up to 1. That’s why it is not a probability distribution. If we would assume <script type="math/tex">w</script> and <script type="math/tex">\sigma^2</script> are given/fixed and we look at it as a function of <script type="math/tex">y_i</script> then it is a proper probability distribution and the integral over <script type="math/tex">y_i</script> does sum of to 1.</p> <h2 id=how-do-we-get-w-and-sigma>How do we get w and sigma?</h2> <p>We are now interested in finding the unknown parameters <script type="math/tex">w</script> and <script type="math/tex">\sigma^2</script>. In order to do so, we have three options.</p> <h3 id=maximum-likelihood-estimation-mle>1. Maximum Likelihood Estimation (MLE)</h3> <p>We assume that our data is identical and independently distributed (aka i.i.d) meaning every data point was generated using the same distribution and all data points are independent of each other. With this assumption we can compute the likelihood function <script type="math/tex">p(y,x|w,\sigma^2) = \prod_{i=1}^{N} p(y_i,x_i|,w,\sigma^2)</script>. Now we can simply maximize this function in order to find the parameter <script type="math/tex">w</script> and <script type="math/tex">\sigma</script> which give us the most likely description for our observed data points.</p> <p>Our task: <script type="math/tex">w*, \sigma^{2*} = \mathop{\arg\,\max}\limits_{w, \sigma^2} \prod_{i=1}^{N} p(y_i,x_i|,w,\sigma^2) = \mathop{\arg\,\max}\limits_{w, \sigma^2} L(w,\sigma^2)</script></p> <p>You might think that we will run into issues when we try to optimize over two variables simultaneously. Generally that can be an issue but not in this case as <script type="math/tex">w^*</script> doesn’t depend on <script type="math/tex">\sigma^2</script>.</p> <script type="math/tex; mode=display">% <![CDATA[
\begin{split}
L(w,\sigma^2) &= log(\prod_{i=1}^{N} p(y_i,x_i|,w,\sigma^2)) \\
&=  \sum\nolimits_{n=1}^N log(p(y_i,x_i|,w,\sigma^2)) \\
&=  \sum\nolimits_{n=1}^N log(\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(y_i - w^Tx_i)^2}{2\sigma^2})) \\
&=  Nlog(\frac{1}{\sqrt{2\pi\sigma^2}} + \sum\nolimits_{n=1}^N -\frac{(y_i - w^Tx_i)^2}{2\sigma^2}) \\
&=  -\frac{N}{2}log(2\pi\sigma^2) - \frac{1}{2\sigma^2} (Xw-y)^{T}(Xw-y)\\
\end{split} %]]></script> <p>We made use of a simple trick. We took the log of the whole expression knowing that it doesn’t change the position of the maximum. In order to find the maximum, we take the gradient with respect to <script type="math/tex">w</script>, set it to zero and solve for <script type="math/tex">w</script>.</p> <script type="math/tex; mode=display">% <![CDATA[
\begin{split}
& \nabla_{w} L(w,\sigma^2) = 0 \\
& \Rightarrow\quad \nabla_{w} [-\frac{N}{2}log(2\pi\sigma^2) - \frac{1}{2\sigma^2} (Xw-y)^{T}(Xw-y)] = 0 \\
& \Rightarrow\quad  \nabla_{w}[- \frac{1}{2\sigma^2} (Xw-y)^{T}(Xw-y)] = 0\\
& \Rightarrow\quad  - \frac{1}{2\sigma^2} \nabla_{w}(Xw-y)^{T}(Xw-y) = 0 \\
& \Rightarrow\quad  - \frac{1}{2\sigma^2} (2XX^Tw-2X^Ty) = 0 \\
& \Rightarrow\quad  XX^Tw-X^Ty = 0 \\
& \Rightarrow\quad  w_{MLE} = (XX^T)^{-1}-X^Ty \\
\end{split} \\ %]]></script> <p>You can see that <script type="math/tex">w_{MLE}</script> is independent of <script type="math/tex">\sigma^2</script> and also that the result is the same as for the non probabilistic least square case with no regularization.</p> <p>Let’s go on to derive the solution for <script type="math/tex">\sigma_{MLE}^2</script>. Same procedure, we take the gradient with respect to <script type="math/tex">\sigma^2</script> , set it to 0 and solve for <script type="math/tex">\sigma^2</script>.</p> <script type="math/tex; mode=display">% <![CDATA[
\begin{split}
& \nabla_{\sigma^2} L(w,\sigma^2) = 0 \\
& \Rightarrow\quad  \nabla_{\sigma^2} [-\frac{N}{2}log(2\pi\sigma^2) - \frac{1}{2\sigma^2} (Xw-y)^{T}(Xw-y)] = 0 \\
& \Rightarrow\quad -\frac{N}{4\pi\sigma^2}2\pi - \frac{1}{4\sigma^4} (Xw-y)^{T}(Xw-y) = 0 \\
& \Rightarrow\quad -N - \frac{1}{2\sigma^2} (Xw-y)^{T}(Xw-y) = 0\\
& \Rightarrow\quad  (Xw-y)^{T}(Xw-y) = N\sigma^2 \\
& \Rightarrow\quad  \sigma_{MLE}^2 = \frac{1}{N}(Xw-y)^{T}(Xw-y) \\
\end{split} %]]></script> <p>This is a satisfying result because it is just finding the sample average of the squared deviations between what <script type="math/tex">w_{MLE}</script> predicts and what the training data actually are. It feels exactly like what happens when you compute the maximum likelihood estimate of the variance of a univariate Gaussian distribution.</p> <p>Let’s quickly recap. The solution for <script type="math/tex">w</script> with the probabilistic approach (+ our assumptions) and taking the Maximum Likelihood Estimate leads exactly to the same solution as for the non probabilistic model. Which means that by using the least squared error function we implictely assume that any noise we have has a mean of zero and is symmetric around the mean. We can’t assume that the noise strictly follows a normal distribution as we would get the same result when the noise would be e.g. uniformly distributed with a mean of 0. But at least we got some better intuition about what’s going on behind the scenes. Let’s go a step further and see if we can also find a corresponding probabilisitc assumption for the regularization term.</p> <p>NOT QUITE SURE YET</p> <h3 id=maximum-a-posteriori-estimation-map>2. Maximum A Posteriori Estimation (MAP)</h3> <p>Bayes formula: <script type="math/tex">\text{A posteriori }= \frac{\text{likelihood * prior}}{\text{evidence}}</script></p> <p>Translated to our model: <script type="math/tex">p(w, \sigma^2|x,y) = \frac{p(x,y|w, \sigma^2) * p(w,\sigma^2)}{p(x,y)}</script></p> <p>The evidence term doesn’t depend on <script type="math/tex">w</script> or <script type="math/tex">\sigma^2</script> and thus doesn’t affect the maximum of the posterior distribution <script type="math/tex">p(w, \sigma^2|x,y)</script> . So in order to find the maximum of the posterior distribution we only need the likelihood function <script type="math/tex">p(x,y|w, \sigma^2)</script> and the prior distribution <script type="math/tex">p(w,\sigma^2)</script>. We already know how to get the likelihood function from the previous step but how do we get the prior?</p> <p>Eventually we need to make some hard assumptions about <script type="math/tex">w</script> and <script type="math/tex">\sigma^2</script>. Let’s discuss our options. We could assume that both <script type="math/tex">w</script> and <script type="math/tex">\sigma^2</script> are indepedent random variables with follow a certain distribution which we can freely choose. For example <script type="math/tex">p(w, \sigma^2) = p(w) * p(\sigma^2) = N(w|a,b)*N(\sigma^2|c,d)</script>. Not much progress made, now we need to come up with good values for <script type="math/tex">a,b,c,d</script>. We could again assume they follow a certain assumptions to take uncertainty into account but you see that we are trapped in a loop. At some point we have a make a hard assumption and set a parameter to a fixed value. It’s good practice too pick a reasonable fixed value when you can’t make good asusmptions about the distribution of a parameter anymore. In our case we will assume <script type="math/tex">\sigma^2</script> is not a random variable but a fixed scalar and <script type="math/tex">w</script> is gaussian distributed with a variance of <script type="math/tex">\frac{1}{\lambda}</script> thus <script type="math/tex">p(w) = N(\sigma^2|0,\frac{1}{\lambda})</script>. Our prior simplifies to <script type="math/tex">p(w, \sigma^2) = N(\sigma^2|0,\frac{1}{\lambda})</script>.</p> <p>No we can ccalculate the maximum a poserior estimation for our only left random variable <script type="math/tex">w</script>.</p> <script type="math/tex; mode=display">% <![CDATA[
\begin{split}
w^* &= \mathop{\arg\,\max}\limits_{w} \text{likelihood * prior} \\
&= \mathop{\arg\,\max}\limits_{w} log(\text{likelihood * prior}) \\
&= - \mathop{\arg\,\min}\limits_{w} log(\text{likelihood * prior}) \\
&= - \mathop{\arg\,\min}\limits_{w} log(L(w)) \\
\end{split} %]]></script> <script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{split}
L(w) &= log(\prod_{i=1}^{N} p(y_i|x_i,w,\sigma^2)) * p(w) \\
&= \sum\nolimits_{n=1}^N log(p(y_i|x_i,w,\sigma^2)) + log(p(w)) \\
&= \sum\nolimits_{n=1}^N log(\frac{1}{\sqrt{2\pi\sigma^2}}exp(-\frac{(y_i - w^Tx_i)^2}{2\sigma^2})) + log(\frac{1}{\sqrt{2\pi\lambda^{-1}}}exp(-\frac{\lambda}{2} (w^Tw)) \\
&= \sum\nolimits_{n=1}^N log(\frac{1}{\sqrt{2\pi\sigma^2}}) - \frac{(y_i - w^Tx_i)^2}{2\sigma^2} + log(\frac{1}{\sqrt{2\pi\lambda^{-1}}}) -\frac{\lambda}{2} (w^Tw)) \\
&= -\frac{N}{2}log(2\pi\sigma^2) -\frac{1}{2}log(2\pi\lambda^{-1}) -\frac{\lambda}{2} (w^Tw)) - \sum\nolimits_{n=1}^N \frac{(y_i - w^Tx_i)^2}{2\sigma^2} \\
&= -\frac{N}{2}log(2\pi\sigma^2) -\frac{1}{2}log(2\pi\lambda^{-1}) -\frac{\lambda}{2} (w^Tw) -\frac{1}{2\sigma^2}(Xw-y)^T(Xw-y)
\end{split}
\end{equation} %]]></script> <p>Now we set the gradient with respect to <script type="math/tex">w</script> to zero and solve for <script type="math/tex">w</script>.</p> <script type="math/tex; mode=display">% <![CDATA[
\begin{split}
& \nabla_{w} L(w) = 0 \\
& \Rightarrow\quad  \nabla_{w} [-\frac{N}{2}log(2\pi\sigma^2) -\frac{1}{2}log(2\pi\lambda^{-1}) -\frac{\lambda}{2} (w^Tw) -\frac{1}{2\sigma^2}(Xw-y)^T(Xw-y)] = 0 \\
& \Rightarrow\quad  \nabla_{w} [-\frac{\lambda}{2} (w^Tw) -\frac{(Xw-y)^T(Xw-y)}{2\sigma^2}] = 0 \\
& \Rightarrow\quad  -\lambda w -\frac{1}{2\sigma^2}  (2X^TXw - 2Xy) = 0 \\
& \Rightarrow\quad  -\lambda w -\frac{1}{\sigma^2}  (X^TXw - Xy) = 0 \\
& \Rightarrow\quad  \lambda \sigma^2 w (X^TXw - Xy) = 0 \\
& \Rightarrow\quad   (X^TX + \lambda \sigma^2 I)w  - Xy = 0 \\
& \Rightarrow\quad   (X^TX + \lambda \sigma^2 I)w  = Xy \\
& \Rightarrow\quad   w^*  = (X^TX + \lambda \sigma^2 I)^{-1} Xy \\
\end{split} %]]></script> <p>Here we get another insight. If you go back to the solution for the optimal <script type="math/tex">w</script> when we used a non probabilistic model with L2 regularization, you see that the result is exactly the same as for the probabilistic model if we assume <script type="math/tex">\sigma^2 =1</script>. Apparently using a <strong>gaussian prior</strong> corresponds to an added L2 reguarization term to our error function. If you play around a bit with different prior distributions you can find out that a <strong>laplacian prior</strong> corresponds to L1 regularization.</p> <p>When I first learnt about these correspondences I was pretty amazed!</p> <h3 id=fully-bayesian-analysis>3. Fully Bayesian Analysis</h3> <p>So far we only did a so called <strong>point estimates</strong> for <script type="math/tex">w</script>. This can work out nicely but we are losing information about the uncertainty of our estimate for <script type="math/tex">w</script>w. If we want to keep the information about the uncertainty we need to compute or estimate the full posterior distribution. I won’t go into more detail here. We will discuss more about this in the next post “Probabilistic Inference - behind the scenes”.</p> <div class=ending> <div id=disqus_thread></div> <script>
//<![CDATA[
var disqus_shortname="blog-janr";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();
//]]>
</script> <noscript>Please enable JavaScript to view the <a href='http://disqus.com/?ref_noscript'>comments powered by Disqus.</a></noscript> <a href='http://disqus.com' class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a> <script>
//<![CDATA[
var disqus_shortname="blog-janr";!function(){var e=document.createElement("script");e.async=!0,e.type="text/javascript",e.src="//"+disqus_shortname+".disqus.com/count.js",(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(e)}();
//]]>
</script> </div> </section> </div> <footer class=footer> </footer> </body> </html>