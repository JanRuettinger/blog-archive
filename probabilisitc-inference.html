<!DOCTYPE html> <html> <head> <link rel=apple-touch-icon-precomposed sizes=152x152 href="apple-touch-icon-152x152-precomposed.png"/> <link rel=apple-touch-icon-precomposed sizes=144x144 href="apple-touch-icon-144x144-precomposed.png"/> <link rel=apple-touch-icon-precomposed sizes=114x114 href="apple-touch-icon-114x114-precomposed.png"/> <link rel=apple-touch-icon-precomposed sizes=72x72 href="apple-touch-icon-72x72-precomposed.png"/> <link rel=apple-touch-icon-precomposed href="apple-touch-icon-precomposed.png"/> <link rel="shortcut icon" href="favicon.png"/> <link rel=icon type="image/ico" href="favicon.ico"/> <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,300' rel=stylesheet> <link rel=stylesheet href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css"> <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script> <meta name=viewport content="width=device-width, initial-scale=1.0"> <link href="stylesheets/main-07333310.css" rel=stylesheet /> <link href="stylesheets/highlighting-33a23ae7.css" rel=stylesheet /> <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120236812-1"></script> <script>
  function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","UA-120236812-1");
</script> </head> <div class=date> 2019-01-18</div> <div class=row> <section class=article-container> <h2 id=how-do-we-get-w-and-sigma>How do we get w and sigma?</h2> <p>We are now interested in finding the unknown parameters <script type="math/tex">w</script> and <script type="math/tex">\sigma^2</script>. In order to do so, we have three options.</p> <ol> <li>Maximum Likelihood Estimation (MLE) We assume that our data is iid meaning every data point was generated using the same distribution and all data points are independent of each other. With this assumption we can compute <script type="math/tex">p(y,x|w,\sigma^2) = \prod_{i=1}^{N} p(y_i,x_i|,w,\sigma^2)</script>. Now we can simply maximize this function in order to find the paramter <script type="math/tex">w</script> and <script type="math/tex">\sigma</script> would provide the most likely description for our observed data points.</li> </ol> <script type="math/tex; mode=display">w*, \sigma^{2*} = {\mathop{\mathrm{arg\,min}}</script> <p>Here we arrived at the first connection between the non probabilitic model from the beginning and the probabilitic model. The MLE solution is exactly the same as the non probabilistic solution. Which means that by using the least squared error function we implictely assume that any noise we have has a mean of zero and is symmetric around the mean. We can’t assume that the noise strictly follows a normal distribution as we would get the same result when the noise would be uniformly distributed with a mean of 0. But at least we got some better intuition about what’s going on behind the scenes. Let’s go a step further and see if we can also find a corresponding probabilisitc assumption for the regularization term.</p> <ol> <li>Maximum A Posteriori Estimation (MAP)</li> </ol> <p>Bayes formula: <script type="math/tex">A\text{ } posteriori = \frac{likelihood * prior}{evidence}</script></p> <p>Translated to our model: <script type="math/tex">p(w, \sigma^2|x,y) = \frac{p(x,y|w, \sigma^2) * p(w,\sigma^2)}{p(x,y)}</script></p> <p>The evidence term doesn’t depend on <script type="math/tex">w</script> or <script type="math/tex">sigma^2</script> and thus doesn’t affect the maximum of the posterior distribution <script type="math/tex">p(w, \sigma^2|x,y)</script>. So in order to find the maximum of the posterior distribution we only need the likelihood function <script type="math/tex">p(x,y|w, \sigma^2)</script> and the prior distribution <script type="math/tex">p(w,\sigma^2)</script>. We already know how to get the likelihood function from the previous step but how do we get the prior?</p> <p>First let’s assume the parameters <script type="math/tex">\sigma</script> and <script type="math/tex">w</script> are indepent thus it follows: <script type="math/tex">p(w,\sigma^2) = p(w)*p(\sigma^2)</script>. Let’s now assume <script type="math/tex">p(w)</script> is normal distributed <script type="math/tex">p(w) = N(0,\lambda)</script> and <script type="math/tex">p(\sigma^2)</script> is uniformly distributed thus it follows <script type="math/tex">p(w,\sigma^2) = N(0,\lambda)</script>.</p> <p>Now we can mutliply the likelihood function and the prior and compute the maximum of this function which will give us the maximum a posteriori estimate for our parameters <script type="math/tex">w</script> and <script type="math/tex">\sigma</script>.</p> <p>Equations.</p> <p>Here we get another insight. Using a <strong>gaussian prior</strong> corresponds to an added L2 reguarization term to our error function. If you play around a bit with different prior distributions you can find out that a <strong>laplacian prior</strong> corresponds to L1 regularization.</p> <p>When I first learnt about these correspondences I was pretty amazed!</p> <ol> <li>Full Bayesian approach</li> </ol> <p>So far we only did so called <strong>point estimates</strong> for <script type="math/tex">w</script> and <script type="math/tex">\sigma^2</script>. This can work out nicely but we are losing information about the uncertainty of our estimate.</p> <p>Let’s look at some examples.</p> <p><img src="images/linear_regression/posterior_mode-2fd3c1ee.png" alt=linear_regression width=1177 height=441 /></p> <p>When we do do a maximum likelihood estimate or a maximum aposteriori estimate we always select the parameter which gives us the maximum value of our mostly unknown posterior distribution. The maximum of a distribution is called <strong>mode</strong>. In the plots below you can see (unknown) posterior distirbutions. When we compute w according to MLE or MAP we don’t know the true postirior distribution we only know a distribution which is proportional to it and which thus has the same maximum.<br/> The first two examples demonstrate cases where using the maximum (green cross) actually makes sense. Even if we knew the full postirior distribution we would pick the same parameter (0 in both cases). The third example however looks different. If we knew the full posterior we would probably wouldn’t pick the argument (-0.75) which gave us the mode of the distribution but rather a value between 0 and 0.75. This seems to be a safer bet. If we would collect a little bit more data then our likelihood function would change slightly so that our previous chosen parameter (-0.75) is now a minimum instead of a maximum. Blindly taking the maximum is a pretty high risk in this case.</p> <p>In order to fully understand what full bayesian approach actually means we need to look into how a final estimate looks depeding on the way we calcualted our unknown parameters.</p> <div class=ending> <div id=disqus_thread></div> <script>
//<![CDATA[
var disqus_shortname="blog-janr";!function(){var e=document.createElement("script");e.type="text/javascript",e.async=!0,e.src="//"+disqus_shortname+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)}();
//]]>
</script> <noscript>Please enable JavaScript to view the <a href='http://disqus.com/?ref_noscript'>comments powered by Disqus.</a></noscript> <a href='http://disqus.com' class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a> <script>
//<![CDATA[
var disqus_shortname="blog-janr";!function(){var e=document.createElement("script");e.async=!0,e.type="text/javascript",e.src="//"+disqus_shortname+".disqus.com/count.js",(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(e)}();
//]]>
</script> </div> </section> </div> <footer class=footer> </footer> </body> </html>